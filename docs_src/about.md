<!-- è‹±æ–‡ç‰ˆå†…å®¹å·²å¤‡æ³¨ -->
# <div align="center">ğŸ§¬ SAGE: ç”¨äºå¤§æ¨¡å‹æ¨ç†çš„åŸç”Ÿæµè®¡ç®—æ¡†æ¶<div>

SAGE æ˜¯ä¸€ä¸ªåŸç”Ÿæ”¯æŒæ•°æ®æµçš„æ•°æ®æ¨ç†æ¡†æ¶ï¼Œä»åº•å±‚è®¾è®¡ä¸Šå°±æ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›æ¨¡å—åŒ–ã€å¯æ§ã€é€æ˜çš„å·¥ä½œæµç¨‹ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºäº LLM çš„ç³»ç»Ÿï¼ˆå¦‚ RAG å’Œæ™ºèƒ½ä½“ï¼‰ä¸­å¸¸è§çš„é—®é¢˜ï¼Œæ¯”å¦‚ç¡¬ç¼–ç çš„ç¼–æ’é€»è¾‘ã€ä¸é€æ˜çš„æ‰§è¡Œè·¯å¾„ï¼Œä»¥åŠæœ‰é™çš„è¿è¡Œæ—¶æ§åˆ¶èƒ½åŠ›ã€‚SAGE å¼•å…¥äº†ä¸€ç§ä»¥æ•°æ®æµä¸ºä¸­å¿ƒçš„æŠ½è±¡æ–¹å¼ï¼Œå°†æ¨ç†æµç¨‹å»ºæ¨¡ä¸ºç”±ç±»å‹åŒ–ç®—å­ç»„æˆçš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œé«˜æ•ˆåœ°æ‰§è¡Œå®æ—¶æ•°æ®å¤„ç†ä»»åŠ¡ã€‚

## å¦‚ä½•ä½¿ç”¨ SAGE æ¡†æ¶ï¼Ÿ

è¯·æŸ¥é˜… [å®‰è£…æŒ‡å—](get_start/install.md) å’Œ [å¿«é€Ÿå¼€å§‹](get_start/quickstart.md)ã€‚

## âœ¨ Features

- **å£°æ˜å¼ä¸æ¨¡å—åŒ–ç»„åˆ**: ä½¿ç”¨ç±»å‹åŒ–ã€å¯å¤ç”¨çš„ç®—å­æ„å»ºå¤æ‚æ¨ç†æµæ°´çº¿ã€‚æ•°æ®æµå›¾æ¸…æ™°åˆ†ç¦»è®¡ç®—å†…å®¹ä¸æ‰§è¡Œæ–¹å¼ã€‚

- **ç»Ÿä¸€çš„æ•°æ®ä¸æ§åˆ¶æµ**: åœ¨å›¾ç»“æ„ä¸­å£°æ˜å¼è¡¨è¾¾æ¡ä»¶åˆ†æ”¯ã€å·¥å…·è·¯ç”±å’Œå›é€€é€»è¾‘ï¼Œæ¶ˆé™¤è„†å¼±çš„å‘½ä»¤å¼æ§åˆ¶ä»£ç ã€‚

- **åŸç”Ÿæœ‰çŠ¶æ€ç®—å­**: å¯å°†ä¼šè¯ã€ä»»åŠ¡å’Œé•¿æœŸè®°å¿†å»ºæ¨¡ä¸ºå›¾ä¸­çš„æœ‰çŠ¶æ€èŠ‚ç‚¹ï¼Œå®ç°æŒä¹…çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è®¡ç®—ã€‚

- **å¼‚æ­¥ä¸”å¼¹æ€§çš„è¿è¡Œæ—¶**: å¼•æ“ä»¥éé˜»å¡ã€æ•°æ®é©±åŠ¨æ–¹å¼å¼‚æ­¥æ‰§è¡Œ DAGï¼Œå…·å¤‡æµæ„ŸçŸ¥é˜Ÿåˆ—ã€äº‹ä»¶é©±åŠ¨è°ƒåº¦å’Œå†…å»ºèƒŒå‹ï¼Œç¨³å¥å¤„ç†å¤æ‚è´Ÿè½½ã€‚

- **å†…å»ºå¯è§‚æµ‹æ€§ä¸è‡ªçœèƒ½åŠ›**: æä¾›äº¤äº’å¼ä»ªè¡¨ç›˜ï¼Œå¼€ç®±å³ç”¨çš„è¿è¡Œæ—¶ç›‘æ§ã€‚æ”¯æŒå¯è§†åŒ–æ‰§è¡Œå›¾ã€ç®—å­çº§æŒ‡æ ‡ç›‘æ§å’Œå®æ—¶æµæ°´çº¿è°ƒè¯•ã€‚

<!-- # <div align="center">ğŸ§¬ SAGE: A Dataflow-Native Framework for LLM Reasoning<div> -->
<!-- SAGE is a dataflow-native reasoning framework built from the ground up to support modular, controllable, and transparent workflows over Large Language Models (LLMs). It addresses common problems in existing LLM-augmented systems (like RAG and Agents), such as hard-coded orchestration logic, opaque execution paths, and limited runtime control. SAGE introduces a dataflow-centric abstraction, modeling reasoning workflows as directed acyclic graphs (DAGs) composed of typed operators.

![](./asset/framework.png)

## âœ¨ Features

- ğŸ§© **Declarative & Modular Composition**: Build complex reasoning pipelines from typed, reusable operators. The dataflow graph cleanly separates what to compute from how to compute it.

- ğŸ”€ **Unified Data and Control Flow**: Express conditional branching, tool routing, and fallback logic declaratively within the graph structure, eliminating brittle, imperative control code.

- ğŸ’¾ **Native Stateful Operators**: Memory is a first-class citizen. Model session, task, and long-term memory as stateful nodes directly within the graph for persistent, context-aware computation.

- âš¡ **Asynchronous & Resilient Runtime**: The engine executes DAGs asynchronously in a non-blocking, data-driven manner. It features stream-aware queues, event-driven scheduling, and built-in backpressure to handle complex workloads gracefully.

- ğŸ“Š **Built-in Observability & Introspection**: An interactive dashboard provides runtime instrumentation out-of-the-box. Visually inspect execution graphs, monitor operator-level metrics, and debug pipeline behavior in real-time.

## ğŸ”§ Installation

To accommodate different user environments and preferences, we provide **comprehensive setup scripts** that support multiple installation modes. Simply run the top-level `./setup.sh` script and choose from the following four installation options:

```bash
./setup.sh
```

You will be prompted to select one of the following modes:

1. **Minimal Setup**  
   Set up only the Conda environment.

   To start with Minimal Setup, you need:

    - Conda (Miniconda or Anaconda)
    - Python â‰¥ 3.11
    - Hugging Face CLI

<!-- 2. **Setup with Ray**  
   Includes the minimal setup and additionally installs [Ray](https://www.ray.io/), a distributed computing framework. -->

<!-- 2. **Setup with Docker**  
   Launches a pre-configured Docker container and sets up the Conda environment inside it.

3. **Full Setup**  
   Launches the Docker container, installs all required dependencies (including **CANDY**, our in-house vector database), and sets up the Conda environment.

---

Alternatively, you can install the project manually:

1. Create a new Conda environment with Python â‰¥ 3.11:

   ```bash
   conda create -n sage python=3.11
   conda activate sage
   ```

2. Install the package from the root directory:

   ```bash
   pip install .
   ```

This method is recommended for advanced users who prefer manual dependency management or wish to integrate the project into existing workflows.




## ğŸš€ Quick Start
### ğŸ§  Memory Toolkit

Memory provides a lightweight in-memory vector database (VDB) supporting text embeddings, vector indexing, multi-index management, metadata filtering, persistence to disk, and recovery.

---

#### (1). Initialize Vector DB and Embedding Model

```python
mgr = MemoryManager()
embedder = MockTextEmbedder(fixed_dim=16)
col = mgr.create_collection(
    name="test_vdb",
    backend_type="VDB",
    description="test VDB",
    embedding_model=embedder,
    dim=16
)
â€‹````


#### (2). Insert Text Entries with Metadata

â€‹```python
col.add_metadata_field("tag")
col.insert("Alpha", {"tag": "A"})
col.insert("Beta", {"tag": "B"})
col.insert("Gamma", {"tag": "A"})
```


#### (3). Create Indexes (e.g., Filtered by Metadata)

```python
col.create_index("global_index")
col.create_index("tag_A_index", metadata_filter_func=lambda m: m.get("tag") == "A")
```

#### (4). Retrieve Similar Vectors

```python
res1 = col.retrieve("Alpha", topk=1, index_name="global_index")
res2 = col.retrieve("Alpha", topk=5, index_name="tag_A_index")
```

#### (5). Persist Collection to Local Disk

```python
mgr.store_collection()
print("Saved to:", mgr.data_dir)
```

#### (6). Reload Persisted Collection (Requires Embedding Model)

```python
mgr2 = MemoryManager()
embedder2 = MockTextEmbedder(fixed_dim=16)
col2 = mgr2.connect_collection("test_vdb", embedding_model=embedder2)
```

#### (7). Delete All Persisted Data (Optional)

```python
VDBMemoryCollection.clear("test_vdb", mgr.data_dir)
manager_json = os.path.join(mgr.data_dir, "manager.json")
if os.path.exists(manager_json):
    os.remove(manager_json)
```

### ğŸ”§ Step-by-Step: Build a Local RAG Pipeline
SAGE uses a **fluent-style API** to declaratively define RAG pipelines. Here's how to get started:

---


```python
from sage_core.api.env import LocalEnvironment
from sage_common_funs.io.source import FileSource
from sage_common_funs.rag.retriever import DenseRetriever
from sage_common_funs.rag.promptor import QAPromptor
from sage_common_funs.rag.generator import OpenAIGenerator
from sage_common_funs.io.sink import TerminalSink
from sage_utils.config_loader import load_config

config = load_config("config.yaml")

env = LocalEnvironment()
env.set_memory(config=None)

query_stream = (env
   .from_source(FileSource, config["source"])
   .map(DenseRetriever, config["retriever"])
   .map(QAPromptor, config["promptor"])
   .map(OpenAIGenerator, config["generator"])
   .sink(TerminalSink, config["sink"])
)

try:
   env.submit()
   env.run_once() 
   time.sleep(5) 
   env.stop()
finally:
   env.close()

```

#### ğŸ“˜ About config

Each operator in the pipeline requires a configuration dictionary config that provides runtime parameters. You can find example config.yaml under [config](./config).

#### ğŸ“˜ About Ray
To enable distributed execution using Ray, you can use RemoteEnvironment.
```python
env = RemoteEnvironment()
```
#### ğŸ“˜ About Long Running
If your pipeline is meant to run as a long-lived service, use:
```python
env.run_streaming() 
```

See more examples under [sage_examples](sage_examples)

## ğŸ§© Components
### Operator
SAGE follows a Flink-style pipeline architecture where each `Operator` acts as a modular and composable processing unit. Operators can be chained together using a fluent API to form a streaming data pipeline. Internally, each `Operator` wraps a stateless or stateful `Function` that defines its core logic.

#### ğŸ”§ Supported Operators
| Operator Method | Description                                                                                                    |
| --------------- | -------------------------------------------------------------------------------------------------------------- |
| `from_source()` | Adds a `SourceFunction` to read input data from external systems.                                              |
| `map()`         | Applies a stateless `Function` to each element of the stream, one-to-one transformation.                       |
| `flatmap()`    | Similar to `map()`, but allows one input to emit zero or more outputs (many-to-many).                          |
| `sink()`        | Defines the terminal output of the stream, consuming the final data (e.g., write to terminal, file, database). |

#### ğŸ”§ Supported Fuction
| Fuction Type        | Description                                                                                                        |
| -------------------- | ------------------------------------------------------------------------------------------------------------------ |
| `SourceOperator`     | Entry point of the pipeline. Ingests input data from external sources such as files, APIs, or user queries.        |
| `RetrievalOperator`  | Performs dense or hybrid retrieval from a vector database or document store based on the input query.              |
| `RerankOperator`     | Reorders retrieved documents using a reranker model (e.g., cross-encoder) to improve relevance.                    |
| `RefineOperator`     | Compresses or filters retrieved context to reduce input length for faster and more accurate model inference.       |
| `PromptOperator`     | Builds model-ready prompts by formatting the query and context into a specific template or structure.              |
| `GenerationOperator` | Generates answers using a large language model (e.g., OpenAI, LLaMA, vLLM) based on the constructed prompt.        |
| `SinkOperator`       | Terminal point of the pipeline. Outputs final results to various sinks like terminal, files, databases, or APIs.   |
| `AgentOperator`      | Enables multi-step decision-making agents that call tools or external APIs based on reasoning strategies.          |
| `EvaluateOperator`   | Calculates metrics like F1, ROUGE, BLEU for model output evaluation. Often used in test/evaluation pipelines.      |
| `RoutingOperator`    | Implements conditional branching or fallback logic within the pipeline (e.g., skip generation if retrieval fails). |

### Memory
![](./asset/Memory_framework.png)

## Engineï¼ˆæ‰§è¡Œå¼•æ“ï¼‰

Sage Engine is the core execution component that orchestrates the compilation and execution of data flow pipelines. It uses a layered architecture to transform logical pipelines into physical execution graphs and efficiently execute them across different runtime environments, supporting both local multi-thread accleration or execution on distributed platrofms.

### How It Works

The Engine operates in four main phases:

1. **Pipeline Collection**: Gathers user-defined logical pipelines built through DataStream API and validates pipeline integrity
2. **Compilation & Optimization**: Uses Compiler to transform logical pipelines into optimized physical execution graphs with parallelism expansion
3. **Runtime Scheduling**: Selects appropriate Runtime (local/distributed) and converts execution graphs into concrete DAG nodes
4. **Execution Monitoring**: Monitors pipeline execution status, collects performance metrics, and handles fault recovery

### Key Features

- **Declarative Programming**: Users describe "what to do", Engine handles "how to do it"
- **Auto-Parallelization**: Automatically determines parallel execution strategies based on data dependencies
- **Platform Agnostic**: Same logical pipeline runs on both local and distributed environments
- **Performance Optimization**: Combines compile-time optimization with runtime tuning
- **Fault Tolerance**: Comprehensive error handling and recovery mechanisms (Under development)

## ğŸ¨ SAGE-Dashboard
<p>With the <strong>SAGE-Dashboard</strong>, you can quickly orchestrate a large model application and run it with one click. Our meticulously designed visual interface will help you efficiently build, monitor, and manage complex workflows!</p>



### âœ¨: Features
- **DAG Visualization**
    - In the dashboard, the running DAG (Directed Acyclic Graph) is rendered in real-time, making your application workflow clear at a glance.</li>
    - Intuitively displays data flows and component dependencies, simplifying the process of understanding complex applications.</li>
- **Live Monitoring**
    - During execution, you can observe the resource usage of various components, including operators and memory, in real-time through the built-in dashboard.</li>
    - Operators are annotated with latency heatmaps, queue occupancy, and runtime statistics. Developers can observe the execution flow in real time, trace performance bottlenecks, and monitor memory behavior.</li>
- **Drag-and-Drop DAG Construction**
    - Quickly assemble a complete DAG workflow by simply arranging and connecting nodes on the canvas, with no need to write complex configuration files.</li>
    - Intuitively define your workflow by dragging and dropping from a rich library of built-in component nodes.</li>

<details>
<summary>Show more</summary>

 <!-- ![](./asset/UI.png) -->
 <!-- <img src="./asset/UI.png" alt="sage-dashboard" width="505"/>
</details>

#### Experience our meticulously designed Sage -Dashboard both user-friendly and powerful::
```bash
cd sage_frontend/sage_server
python main.py --host 127.0.0.1 --port 8080 --log-level debug

cd ../dashboard
npm i 
npm start
``` -->

## ğŸ”– License
SAGE is licensed under the [MIT License](./LICENSE). 
