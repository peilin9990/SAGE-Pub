# LLM QA å¿«é€Ÿå…¥é—¨

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ SAGE æ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é—®ç­”ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸å¸¦æ£€ç´¢çš„ç›´æ¥é—®ç­”å’Œé›†æˆæ£€ç´¢åŠŸèƒ½çš„å¢å¼ºé—®ç­”ã€‚

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

LLM QAï¼ˆå¤§è¯­è¨€æ¨¡å‹é—®ç­”ï¼‰æ˜¯ç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé—®ç­”çš„æŠ€æœ¯ï¼Œæ— éœ€å¤–éƒ¨çŸ¥è¯†åº“æ£€ç´¢ã€‚é€‚ç”¨äºï¼š

- ä¸€èˆ¬å¸¸è¯†é—®ç­”
- ç¼–ç¨‹è¾…åŠ©
- æ–‡æœ¬åˆ›ä½œ
- å®æ—¶å¯¹è¯

---

## ğŸš€ ä¸å¸¦æ£€ç´¢çš„QAç¤ºä¾‹

### ç»ˆç«¯äº¤äº’å¼QA

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªç»ˆç«¯äº¤äº’å¼çš„é—®ç­”ç³»ç»Ÿï¼š

```python
"""
ç»ˆç«¯äº¤äº’å¼QAæ— ç•Œæµå¤„ç†
æ”¯æŒç»ˆç«¯è¾“å…¥é—®é¢˜ï¼Œä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”çš„æ— ç•Œæµå¤„ç†ç¤ºä¾‹
"""
import time
from dotenv import load_dotenv
from sage.common.utils.logging.custom_logger import CustomLogger
from sage.core.api.local_environment import LocalEnvironment
from sage.core.api.function.map_function import MapFunction
from sage.core.api.function.sink_function import SinkFunction
from sage.core.api.function.source_function import SourceFunction
from sage.libs.rag.generator import OpenAIGenerator
from sage.libs.rag.promptor import QAPromptor
from sage.common.utils.config.loader import load_config


class TerminalInputSource(SourceFunction):
    """ç»ˆç«¯è¾“å…¥æºå‡½æ•°"""
    def execute(self, data=None):
        try:
            print("ğŸ¤” è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆæŒ‰Ctrl+Cé€€å‡ºï¼‰:")
            user_input = input(">>> ").strip()
            if user_input:
                return user_input
            return self.execute(data)
        except (EOFError, KeyboardInterrupt):
            raise


class QuestionProcessor(MapFunction):
    """é—®é¢˜å¤„ç†å™¨"""
    def execute(self, data):
        if not data or data.strip() == "":
            return None

        question = data.strip()
        return question


class AnswerFormatter(MapFunction):
    """å›ç­”æ ¼å¼åŒ–å™¨"""
    def execute(self, data):
        if not data:
            return None

        # OpenAIGeneratorè¿”å›çš„æ ¼å¼æ˜¯ (user_query, generated_text)
        if isinstance(data, tuple) and len(data) >= 2:
            user_query = data[0]
            answer = data[1]
            return {
                "question": user_query if user_query else "N/A",
                "answer": answer,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        else:
            return {
                "question": "N/A",
                "answer": str(data),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }


class ConsoleSink(SinkFunction):
    """æ§åˆ¶å°è¾“å‡º"""
    def execute(self, data):
        if not data:
            return None

        if isinstance(data, dict):
            print(f"\nğŸ¤– {data.get('answer', 'N/A')}\n")
        else:
            print(f"\nğŸ¤– {data}\n")

        return data


def create_qa_pipeline():
    """åˆ›å»ºQAå¤„ç†ç®¡é“"""
    import os
    # åŠ è½½é…ç½®
    load_dotenv(override=False)
    config_path = os.path.join(os.path.dirname(__file__), "..", "config", "config_source.yaml")
    config = load_config(config_path)

    # åˆ›å»ºæœ¬åœ°ç¯å¢ƒ
    env = LocalEnvironment()

    # å¯åŠ¨æ¬¢è¿æç¤º
    print("ğŸ’¬ QAåŠ©æ‰‹å·²å¯åŠ¨ï¼è¾“å…¥é—®é¢˜åæŒ‰å›è½¦")

    try:
        # æ„å»ºæ— ç•Œæµå¤„ç†ç®¡é“
        (env
            .from_source(TerminalInputSource)
            .map(QuestionProcessor)
            .map(QAPromptor, config["promptor"])
            .map(OpenAIGenerator, config["generator"]["vllm"])
            .map(AnswerFormatter)
            .sink(ConsoleSink)
        )

        # å¯åŠ¨ç®¡é“
        env.submit()

        # ä¿æŒç¨‹åºè¿è¡Œ
        print("ç¨‹åºè¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C é€€å‡º...")
        import time
        while True:
            time.sleep(1)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
    finally:
        try:
            env.close()
        except:
            pass


if __name__ == "__main__":
    # å…³é—­è°ƒè¯•æ—¥å¿—
    CustomLogger.disable_global_console_debug()
    create_qa_pipeline()
```

### æ‰¹å¤„ç†QA

å¤„ç†é¢„å®šä¹‰çš„é—®é¢˜åˆ—è¡¨ï¼š

```python
from sage.core.api.local_environment import LocalEnvironment
from sage.core.api.function.batch_function import BatchFunction
from sage.libs.rag.generator import OpenAIGenerator
from sage.libs.rag.promptor import QAPromptor
from sage.libs.io_utils.sink import TerminalSink

class QABatch(BatchFunction):
    """QAæ‰¹å¤„ç†æ•°æ®æº"""
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.questions = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "Pythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ"
        ]
        self.counter = 0

    def execute(self):
        if self.counter >= len(self.questions):
            return None
        
        question = self.questions[self.counter]
        self.counter += 1
        return question

def batch_qa_pipeline():
    """æ‰¹å¤„ç†QAç®¡é“"""
    env = LocalEnvironment("batch_qa")
    
    config = {
        "promptor": {"platform": "local"},
        "generator": {
            "vllm": {
                "api_key": "your-api-key",
                "method": "openai",
                "model_name": "gpt-3.5-turbo",
                "base_url": "https://api.openai.com/v1"
            }
        },
        "sink": {"platform": "local"}
    }
    
    (env
        .from_batch(QABatch, config)
        .map(QAPromptor, config["promptor"])
        .map(OpenAIGenerator, config["generator"]["vllm"])
        .sink(TerminalSink, config["sink"])
    )
    
    env.submit(autostop=True)

if __name__ == "__main__":
    batch_qa_pipeline()
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

### åŸºç¡€é…ç½®æ–‡ä»¶

åˆ›å»º `config.yaml` æ–‡ä»¶ï¼š

```yaml
pipeline:
  name: "llm-qa-pipeline"
  description: "LLMé—®ç­”ç®¡é“"

promptor:
  platform: "local"

generator:
  vllm:
    api_key: "your-api-key"
    method: "openai"
    model_name: "gpt-3.5-turbo"
    base_url: "https://api.openai.com/v1"
    temperature: 0.7
    max_tokens: 1000

sink:
  platform: "local"
```

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# OpenAI APIé…ç½®
OPENAI_API_KEY=your-openai-api-key
OPENAI_BASE_URL=https://api.openai.com/v1

# æˆ–è€…ä½¿ç”¨å…¼å®¹çš„API
VLLM_API_KEY=your-vllm-api-key
VLLM_BASE_URL=http://localhost:8000/v1
```

---

## ğŸ¨ è‡ªå®šä¹‰æç¤ºè¯

### åŸºç¡€æç¤ºè¯æ¨¡æ¿

```python
from jinja2 import Template

# è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
qa_template = Template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{% if external_corpus %}
å‚è€ƒä¿¡æ¯ï¼š
{{ external_corpus }}
{% endif %}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚
""")

# åœ¨QAPromptorä¸­ä½¿ç”¨
config = {
    "promptor": {
        "platform": "local",
        "template": qa_template
    }
}
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
class OptimizedQABatch(BatchFunction):
    """ä¼˜åŒ–çš„æ‰¹å¤„ç†QA"""
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = config.get("batch_size", 5)
        self.questions = self.load_questions(config["data_path"])
        self.current_batch = 0
        
    def execute(self):
        start_idx = self.current_batch * self.batch_size
        end_idx = start_idx + self.batch_size
        
        if start_idx >= len(self.questions):
            return None
            
        batch_questions = self.questions[start_idx:end_idx]
        self.current_batch += 1
        return batch_questions
```

### å¼‚æ­¥å¤„ç†

```python
import asyncio
from sage.core.api.function.map_function import MapFunction

class AsyncQAProcessor(MapFunction):
    """å¼‚æ­¥QAå¤„ç†å™¨"""
    async def async_execute(self, data):
        # å¼‚æ­¥è°ƒç”¨LLM API
        result = await self.async_llm_call(data)
        return result
    
    def execute(self, data):
        # åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(self.async_execute(data))
```

---

## ğŸ” é”™è¯¯å¤„ç†

### å¥å£®çš„é”™è¯¯å¤„ç†

```python
class RobustQAProcessor(MapFunction):
    """å¸¦é”™è¯¯å¤„ç†çš„QAå¤„ç†å™¨"""
    def execute(self, data):
        try:
            # å¤„ç†é—®é¢˜
            if not data or not data.strip():
                return {"error": "ç©ºé—®é¢˜", "question": data}
                
            # è°ƒç”¨LLM
            result = self.call_llm(data)
            return {"success": True, "question": data, "answer": result}
            
        except Exception as e:
            self.logger.error(f"QAå¤„ç†é”™è¯¯: {e}")
            return {
                "error": str(e),
                "question": data,
                "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
            }
```

---

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### æ€§èƒ½ç›‘æ§

```python
import time
from collections import defaultdict

class QAMonitorSink(SinkFunction):
    """QAç›‘æ§è¾“å‡º"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.stats = defaultdict(int)
        self.start_time = time.time()
    
    def execute(self, data):
        self.stats["total_questions"] += 1
        
        if isinstance(data, dict):
            if "error" in data:
                self.stats["errors"] += 1
            else:
                self.stats["success"] += 1
                
        # æ¯100ä¸ªé—®é¢˜æ‰“å°ä¸€æ¬¡ç»Ÿè®¡
        if self.stats["total_questions"] % 100 == 0:
            self.print_stats()
            
        return data
    
    def print_stats(self):
        elapsed = time.time() - self.start_time
        print(f"å¤„ç†ç»Ÿè®¡: {self.stats['total_questions']} é—®é¢˜")
        print(f"æˆåŠŸç‡: {self.stats['success'] / self.stats['total_questions'] * 100:.1f}%")
        print(f"å¹³å‡é€Ÿåº¦: {self.stats['total_questions'] / elapsed:.1f} é—®é¢˜/ç§’")
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. **å®‰è£…ä¾èµ–**ï¼šç¡®ä¿å·²å®‰è£… SAGE å’Œç›¸å…³ä¾èµ–
2. **é…ç½®API**ï¼šè®¾ç½®å¤§è¯­è¨€æ¨¡å‹çš„APIå¯†é’¥å’Œåœ°å€
3. **è¿è¡Œç¤ºä¾‹**ï¼šé€‰æ‹©åˆé€‚çš„ç¤ºä¾‹ä»£ç è¿è¡Œ
4. **è‡ªå®šä¹‰æ‰©å±•**ï¼šæ ¹æ®éœ€æ±‚ä¿®æ”¹æç¤ºè¯å’Œå¤„ç†é€»è¾‘

---

*é€šè¿‡SAGEæ„å»ºçš„LLM QAç³»ç»Ÿå…·å¤‡é«˜åº¦çš„çµæ´»æ€§å’Œæ‰©å±•æ€§ï¼Œæ˜¯æ™ºèƒ½é—®ç­”åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚*
