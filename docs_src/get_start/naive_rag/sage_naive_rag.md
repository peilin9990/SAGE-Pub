# Naive RAGå®ç°æŒ‡å—

> æœ¬æ–‡æ¡£é¢å‘ç†Ÿæ‚‰RAGåŸºæœ¬æµç¨‹çš„å¼€å‘è€…ã€‚å¦‚æœæ‚¨å¯¹RAGæ¦‚å¿µä¸äº†è§£ï¼Œè¯·å…ˆé˜…è¯» [RAG - æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ä»‹ç»](you_know_rag.md)ã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹©SAGEæ„å»ºRAGï¼Ÿ

SAGEæµå¤„ç†æ¡†æ¶ä¸ºRAGç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æŠ€æœ¯åŸºç¡€ï¼š

- **ç»Ÿä¸€çš„æ•°æ®æµå¤„ç†**ï¼šæ”¯æŒæ‰¹å¤„ç†å’Œæµå¼å¤„ç†ä¸¤ç§æ¨¡å¼
- **ä¸°å¯Œçš„RAGç»„ä»¶åº“**ï¼šæä¾›æ£€ç´¢å™¨ã€ç”Ÿæˆå™¨ã€æç¤ºè¯ç­‰é¢„åˆ¶ç»„ä»¶
- **çµæ´»çš„æœåŠ¡é›†æˆ**ï¼šæ”¯æŒå†…å­˜æœåŠ¡ã€å‘é‡æ•°æ®åº“ç­‰æœåŠ¡æ³¨å†Œ
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šå¯çµæ´»ç»„åˆä¸åŒçš„å¤„ç†ç®—å­

## Naive RAG Pipelineç¤ºä¾‹

æˆ‘ä»¬å°†é€šè¿‡ SAGE ä¸­é¢„å®šä¹‰çš„ç®—å­å‘æ‚¨å±•ç¤ºä¸€ä¸ªå®Œæ•´çš„ Naive RAG æŸ¥è¯¢å¤„ç†æµæ°´çº¿çš„åˆ›å»ºï¼š

```python
from sage.core.api.local_environment import LocalEnvironment
from sage.libs.io_utils.source import FileSource
from sage.libs.io_utils.sink import TerminalSink
from sage.libs.rag.retriever import DenseRetriever
from sage.libs.rag.promptor import QAPromptor
from sage.libs.rag.generator import OpenAIGenerator
from sage.common.utils.config.loader import load_config

def naive_rag_pipeline():
    """åˆ›å»ºNaive RAGå¤„ç†ç®¡é“"""
    env = LocalEnvironment("naive_rag")
    
    # åŠ è½½é…ç½®
    config = load_config("examples/config/config.yaml")
    
    # æ„å»ºRAGå¤„ç†æµç¨‹
    (env
        .from_source(FileSource, config["source"])
        .map(DenseRetriever, config["retriever"])
        .map(QAPromptor, config["promptor"])
        .map(OpenAIGenerator, config["generator"]["vllm"])
        .sink(TerminalSink, config["sink"])
    )
    
    # æäº¤å¹¶è¿è¡Œ
    env.submit()
    
    # ç­‰å¾…å¤„ç†å®Œæˆ
    import time
    time.sleep(10)
    env.close()

if __name__ == '__main__':
    naive_rag_pipeline()
```

åœ¨å®Œæˆäº†åŸºç¡€çš„ Naive RAG Pipeline æ„å»ºåï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å‡†å¤‡ç¦»çº¿çŸ¥è¯†åº“ï¼Œä»¥ä¾¿ä¸ºæŸ¥è¯¢æä¾›å¯æ£€ç´¢çš„ä¿¡æ¯æ”¯æŒã€‚ä»¥ä¸‹æ˜¯çŸ¥è¯†åº“æ„å»ºçš„å…³é”®æ­¥éª¤ä¸ç¤ºä¾‹ä»£ç ï¼š

## åŸºäºChromaçš„çŸ¥è¯†åº“æ„å»º

### 1. æ„å»ºå‘é‡ç´¢å¼•

ä½¿ç”¨SAGEæä¾›çš„Chromaç´¢å¼•æ„å»ºå·¥å…·ï¼š

```python
"""
ä½¿ç”¨Chromaæ„å»ºå‘é‡ç´¢å¼•çš„ç¤ºä¾‹
"""
import os
from sage.libs.rag.retriever import ChromaRetriever
from sage.middleware.utils.embedding.embedding_api import apply_embedding_model
from sage.common.utils.config.loader import load_config

def build_chroma_index():
    """æ„å»ºChromaå‘é‡ç´¢å¼•"""
    
    # åŠ è½½é…ç½®
    config_path = os.path.join(os.path.dirname(__file__), "..", "config", "config_qa_chroma.yaml")
    config = load_config(config_path)
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embedding_model = apply_embedding_model("default")
    
    # å‡†å¤‡çŸ¥è¯†æ•°æ®
    knowledge_data = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†å›¾åƒå’Œè¯­éŸ³ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åº”ç”¨é¢†åŸŸï¼Œæ¶‰åŠæ–‡æœ¬ç†è§£ã€ç”Ÿæˆå’Œç¿»è¯‘ç­‰ä»»åŠ¡ã€‚",
        "å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨æ¸¸æˆå’Œæœºå™¨äººæ§åˆ¶ä¸­åº”ç”¨å¹¿æ³›ã€‚",
        "è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿç†è§£å’Œå¤„ç†è§†è§‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰ã€‚"
    ]
    
    # ä½¿ç”¨ChromaRetrieveræ„å»ºç´¢å¼•
    retriever = ChromaRetriever(config["retriever"])
    
    # å‘ç´¢å¼•ä¸­æ·»åŠ æ–‡æ¡£
    for i, text in enumerate(knowledge_data):
        retriever.add_document(
            document_id=f"doc_{i}",
            text=text,
            metadata={"source": "knowledge_base", "topic": "AI/ML"}
        )
    
    print("âœ… ChromaçŸ¥è¯†åº“æ„å»ºå®Œæˆ")
    return retriever

if __name__ == "__main__":
    build_chroma_index()
```

### 2. RAGæŸ¥è¯¢å¤„ç†

åŸºäºæ„å»ºçš„çŸ¥è¯†åº“è¿›è¡ŒæŸ¥è¯¢ï¼š

```python
"""
ä½¿ç”¨Chromaè¿›è¡ŒRAGæŸ¥è¯¢çš„å®Œæ•´ç¤ºä¾‹
"""
from sage.core.api.local_environment import LocalEnvironment
from sage.core.api.function.batch_function import BatchFunction
from sage.libs.rag.retriever import ChromaRetriever
from sage.libs.rag.promptor import QAPromptor
from sage.libs.rag.generator import OpenAIGenerator
from sage.libs.io_utils.sink import TerminalSink

class QueryBatch(BatchFunction):
    """æŸ¥è¯¢æ‰¹å¤„ç†æ•°æ®æº"""
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.queries = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
            "å¼ºåŒ–å­¦ä¹ å¦‚ä½•å·¥ä½œï¼Ÿ"
        ]
        self.counter = 0

    def execute(self):
        if self.counter >= len(self.queries):
            return None
        
        query = self.queries[self.counter]
        self.counter += 1
        return query

def chroma_rag_pipeline():
    """ä½¿ç”¨Chromaçš„RAGç®¡é“"""
    env = LocalEnvironment("chroma_rag")
    
    # é…ç½®
    config = {
        "source": {"data_path": "examples/data/sample/question.txt"},
        "retriever": {
            "platform": "local",
            "chroma": {
                "collection_name": "knowledge_base",
                "persist_directory": "./chroma_db",
                "top_k": 3
            }
        },
        "promptor": {"platform": "local"},
        "generator": {
            "vllm": {
                "api_key": "your-api-key",
                "method": "openai",
                "model_name": "gpt-3.5-turbo",
                "base_url": "https://api.openai.com/v1"
            }
        },
        "sink": {"platform": "local"}
    }
    
    (env
        .from_batch(QueryBatch, config["source"])
        .map(ChromaRetriever, config["retriever"])
        .map(QAPromptor, config["promptor"])
        .map(OpenAIGenerator, config["generator"]["vllm"])
        .sink(TerminalSink, config["sink"])
    )
    
    env.submit(autostop=True)

if __name__ == "__main__":
    chroma_rag_pipeline()
```

## åŸºäºMilvusçš„åˆ†å¸ƒå¼RAG

### 1. Milvusç´¢å¼•æ„å»º

```python
"""
ä½¿ç”¨Milvusæ„å»ºåˆ†å¸ƒå¼å‘é‡ç´¢å¼•
"""
from sage.libs.rag.retriever import MilvusRetriever
from sage.middleware.utils.embedding.embedding_api import apply_embedding_model

def build_milvus_index():
    """æ„å»ºMilvuså‘é‡ç´¢å¼•"""
    
    config = {
        "retriever": {
            "platform": "local",
            "milvus": {
                "host": "localhost",
                "port": 19530,
                "collection_name": "knowledge_collection",
                "dimension": 384,
                "top_k": 5,
                "metric_type": "L2"
            }
        }
    }
    
    # åˆå§‹åŒ–Milvusæ£€ç´¢å™¨
    retriever = MilvusRetriever(config["retriever"])
    
    # å‡†å¤‡æ–‡æ¡£æ•°æ®
    documents = [
        {"id": 1, "text": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦ã€‚", "metadata": {"category": "programming"}},
        {"id": 2, "text": "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚", "metadata": {"category": "ai"}},
        {"id": 3, "text": "Dockerå®¹å™¨åŒ–æŠ€æœ¯ç®€åŒ–äº†åº”ç”¨éƒ¨ç½²ã€‚", "metadata": {"category": "devops"}},
    ]
    
    # æ‰¹é‡æ’å…¥æ–‡æ¡£
    retriever.bulk_insert(documents)
    
    print("âœ… MilvusçŸ¥è¯†åº“æ„å»ºå®Œæˆ")

if __name__ == "__main__":
    build_milvus_index()
```

### 2. åˆ†å¸ƒå¼RAGç®¡é“

```python
"""
åŸºäºMilvusçš„åˆ†å¸ƒå¼RAGç®¡é“
"""
from sage.core.api.remote_environment import RemoteEnvironment
from sage.libs.io_utils.source import FileSource
from sage.libs.rag.retriever import MilvusRetriever
from sage.libs.rag.promptor import QAPromptor
from sage.libs.rag.generator import OpenAIGenerator
from sage.libs.io_utils.sink import TerminalSink

def distributed_rag_pipeline():
    """åˆ†å¸ƒå¼RAGç®¡é“"""
    # è¿æ¥åˆ°è¿œç¨‹JobManager
    env = RemoteEnvironment(
        name="distributed_rag",
        host="127.0.0.1",
        port=19001
    )
    
    config = {
        "source": {"data_path": "examples/data/sample/question.txt"},
        "retriever": {
            "platform": "local",
            "milvus": {
                "host": "milvus-server",
                "port": 19530,
                "collection_name": "knowledge_collection",
                "top_k": 3
            }
        },
        "promptor": {"platform": "local"},
        "generator": {
            "vllm": {
                "api_key": "your-api-key",
                "method": "openai",
                "model_name": "gpt-3.5-turbo",
                "base_url": "http://llm-server:8000/v1"
            }
        },
        "sink": {"platform": "local"}
    }
    
    (env
        .from_source(FileSource, config["source"])
        .map(MilvusRetriever, config["retriever"])
        .map(QAPromptor, config["promptor"])
        .map(OpenAIGenerator, config["generator"]["vllm"])
        .sink(TerminalSink, config["sink"])
    )
    
    env.submit()

if __name__ == "__main__":
    distributed_rag_pipeline()
```

## æµå¼RAGæŸ¥è¯¢å¤„ç†

### 1. å®æ—¶æŸ¥è¯¢å¤„ç†æµæ°´çº¿

SAGEæ”¯æŒæ„å»ºæµå¼RAGç³»ç»Ÿï¼Œå¯ä»¥å®æ—¶å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼š

```python
"""
æµå¼RAGæŸ¥è¯¢å¤„ç†ç¤ºä¾‹
"""
from sage.core.api.local_environment import LocalEnvironment
from sage.core.api.function.source_function import SourceFunction
from sage.core.api.function.map_function import MapFunction
from sage.core.api.function.sink_function import SinkFunction
import time

class StreamingQuerySource(SourceFunction):
    """æ¨¡æ‹Ÿå®æ—¶æŸ¥è¯¢æµ"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.test_queries = [
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "Pythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ", 
            "Dockerå¦‚ä½•ä½¿ç”¨ï¼Ÿ",
            "æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯ï¼Ÿ"
        ]
        self.counter = 0

    def execute(self):
        if self.counter >= len(self.test_queries):
            self.counter = 0  # å¾ªç¯æŸ¥è¯¢
        
        query_data = {
            "query": self.test_queries[self.counter],
            "query_id": self.counter + 1,
            "timestamp": time.time()
        }
        self.counter += 1
        return query_data

class KnowledgeRetriever(MapFunction):
    """çŸ¥è¯†æ£€ç´¢ç®—å­"""
    def __init__(self, config, **kwargs):
        super().__init__(**kwargs)
        self.config = config
        self.topk = config.get("topk", 3)

    def execute(self, data):
        query = data.get("query", "")
        
        # æ¨¡æ‹Ÿæ£€ç´¢è¿‡ç¨‹
        retrieved_docs = [
            f"ç›¸å…³æ–‡æ¡£1ï¼šå…³äº'{query}'çš„åŸºç¡€çŸ¥è¯†",
            f"ç›¸å…³æ–‡æ¡£2ï¼š'{query}'çš„å®é™…åº”ç”¨",
            f"ç›¸å…³æ–‡æ¡£3ï¼š'{query}'çš„è¿›é˜¶å†…å®¹"
        ]
        
        data["retrieved_knowledge"] = retrieved_docs
        return data

class StreamingRAGSink(SinkFunction):
    """æµå¼RAGç»“æœè¾“å‡º"""
    def execute(self, data):
        print(f"\nğŸ” æŸ¥è¯¢ {data.get('query_id', 'N/A')}: {data.get('query', 'N/A')}")
        
        if "answer" in data:
            print(f"ğŸ¤– å›ç­”: {data['answer']}")
        
        if "retrieved_knowledge" in data:
            print(f"ğŸ“– æ£€ç´¢åˆ° {len(data['retrieved_knowledge'])} æ¡ç›¸å…³çŸ¥è¯†")
        
        print("â”€" * 50)
        return data

def streaming_rag_pipeline():
    """åˆ›å»ºæµå¼RAGå¤„ç†ç¯å¢ƒ"""
    env = LocalEnvironment("streaming_rag")
    
    config = {
        "retriever": {"topk": 3},
        "promptor": {"platform": "local"},
        "generator": {
            "vllm": {
                "api_key": "your-api-key",
                "method": "openai", 
                "model_name": "gpt-3.5-turbo",
                "base_url": "https://api.openai.com/v1"
            }
        }
    }
    
    # æ„å»ºæµæ°´çº¿
    (env
        .from_source(StreamingQuerySource)
        .map(KnowledgeRetriever, config["retriever"])
        .map(QAPromptor, config["promptor"])
        .map(OpenAIGenerator, config["generator"]["vllm"])
        .sink(StreamingRAGSink)
    )
    
    try:
        print("ğŸš€ å¯åŠ¨æµå¼RAGç³»ç»Ÿ...")
        env.submit()
        
        # è¿è¡Œä¸€æ®µæ—¶é—´
        import time
        time.sleep(30)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åœæ­¢æµå¼RAGç³»ç»Ÿ")
    finally:
        env.close()

if __name__ == "__main__":
    streaming_rag_pipeline()
```

## ç³»ç»Ÿç‰¹æ€§ä¸ä¼˜åŠ¿

### æ ¸å¿ƒç»„ä»¶ç‰¹æ€§
- **çµæ´»çš„æ£€ç´¢å™¨**ï¼šæ”¯æŒChromaã€Milvusç­‰å¤šç§å‘é‡æ•°æ®åº“
- **æ™ºèƒ½æç¤ºè¯ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç»„åˆæŸ¥è¯¢å’Œæ£€ç´¢ä¸Šä¸‹æ–‡
- **å¤šæ¨¡å‹ç”Ÿæˆå™¨**ï¼šæ”¯æŒOpenAIã€vLLMç­‰å¤šç§ç”Ÿæˆæ¨¡å‹
- **æµå¼å¤„ç†èƒ½åŠ›**ï¼šæ”¯æŒå®æ—¶å’Œæ‰¹é‡ä¸¤ç§å¤„ç†æ¨¡å¼

### æ€§èƒ½ä¼˜åŠ¿
- **é«˜æ•ˆæ£€ç´¢**ï¼šæ¯«ç§’çº§å‘é‡ç›¸ä¼¼åº¦æœç´¢
- **å¼¹æ€§æ‰©å±•**ï¼šæ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²å’Œè´Ÿè½½å‡è¡¡
- **å®¹é”™å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **ç›‘æ§è§‚æµ‹**ï¼šå…¨é“¾è·¯å¤„ç†çŠ¶æ€å¯è§‚æµ‹

## é…ç½®ç¤ºä¾‹

### å®Œæ•´é…ç½®æ–‡ä»¶

```yaml
pipeline:
  name: "naive-rag-pipeline"
  description: "åŸºç¡€RAGé—®ç­”ç®¡é“"

source:
  data_path: "examples/data/sample/question.txt"
  platform: "local"

retriever:
  platform: "local"
  # Chromaé…ç½®
  chroma:
    collection_name: "knowledge_base"
    persist_directory: "./chroma_db"
    top_k: 3
    
  # æˆ–è€…Milvusé…ç½®
  milvus:
    host: "localhost"
    port: 19530
    collection_name: "knowledge_collection"
    dimension: 384
    top_k: 5
    metric_type: "L2"

promptor:
  platform: "local"
  template_type: "qa"  # æˆ– "summarization"

generator:
  vllm:
    api_key: "your-api-key"
    method: "openai"
    model_name: "gpt-3.5-turbo"
    base_url: "https://api.openai.com/v1"
    temperature: 0.7
    max_tokens: 1000

sink:
  platform: "local"
```

## æ‰©å±•æ–¹å‘

1. **é«˜çº§æ£€ç´¢æŠ€æœ¯**ï¼š
   - æ··åˆæ£€ç´¢ï¼ˆå¯†é›†+ç¨€ç–ï¼‰
   - é‡æ’åºï¼ˆRerankingï¼‰
   - æŸ¥è¯¢æ‰©å±•å’Œæ”¹å†™

2. **æç¤ºè¯å·¥ç¨‹**ï¼š
   - åŠ¨æ€æç¤ºè¯æ¨¡æ¿
   - ä¸Šä¸‹æ–‡é•¿åº¦ä¼˜åŒ–
   - å¤šè½®å¯¹è¯æ”¯æŒ

3. **ç”Ÿæˆè´¨é‡ä¼˜åŒ–**ï¼š
   - ç­”æ¡ˆè´¨é‡è¯„ä¼°
   - å¼•ç”¨å’Œæº¯æº
   - å¹»è§‰æ£€æµ‹å’Œç¼“è§£

4. **ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–**ï¼š
   - ç¼“å­˜æœºåˆ¶
   - æ‰¹é‡å¤„ç†
   - å¼‚æ­¥å¹¶å‘

## æœ€ä½³å®è·µ

### 1. çŸ¥è¯†åº“è´¨é‡
- ç¡®ä¿æ–‡æ¡£å†…å®¹çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§
- åˆç†çš„æ–‡æ¡£åˆ†å—ç­–ç•¥
- ä¸°å¯Œçš„å…ƒæ•°æ®æ ‡æ³¨

### 2. æ£€ç´¢ä¼˜åŒ–
- é€‰æ‹©åˆé€‚çš„embeddingæ¨¡å‹
- è°ƒä¼˜æ£€ç´¢å‚æ•°ï¼ˆtop_kã€é˜ˆå€¼ç­‰ï¼‰
- å®æ–½æ£€ç´¢ç»“æœè¿‡æ»¤å’Œæ’åº

### 3. ç”Ÿæˆä¼˜åŒ–
- è®¾è®¡æœ‰æ•ˆçš„æç¤ºè¯æ¨¡æ¿
- è°ƒèŠ‚æ¨¡å‹å‚æ•°ï¼ˆtemperatureã€top_pç­‰ï¼‰
- å®æ–½è¾“å‡ºæ ¼å¼æ§åˆ¶

### 4. ç³»ç»Ÿç›‘æ§
- æ£€ç´¢ç²¾åº¦ç›‘æ§
- ç”Ÿæˆè´¨é‡è¯„ä¼°
- ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡è¿½è¸ª

## å¿«é€Ÿå¼€å§‹

1. **ç¯å¢ƒå‡†å¤‡**ï¼š
   ```bash
   # å®‰è£…SAGE
   pip install sage-framework
   
   # å¯åŠ¨å‘é‡æ•°æ®åº“ï¼ˆé€‰æ‹©å…¶ä¸€ï¼‰
   docker run -p 19530:19530 milvusdb/milvus:latest  # Milvus
   # æˆ–è€…ç›´æ¥ä½¿ç”¨Chromaï¼ˆæ— éœ€é¢å¤–å®‰è£…ï¼‰
   ```

2. **æ„å»ºçŸ¥è¯†åº“**ï¼š
   ```bash
   python examples/rag/build_chroma_index.py
   # æˆ–
   python examples/rag/build_milvus_index.py
   ```

3. **è¿è¡ŒRAGç®¡é“**ï¼š
   ```bash
   python examples/rag/qa_dense_retrieval_chroma.py
   # æˆ–
   python examples/rag/qa_dense_retrieval_milvus.py
   ```

4. **è‡ªå®šä¹‰æ‰©å±•**ï¼š
   - æ ¹æ®éœ€æ±‚ä¿®æ”¹é…ç½®æ–‡ä»¶
   - è‡ªå®šä¹‰æ£€ç´¢å’Œç”Ÿæˆç»„ä»¶
   - é›†æˆä¸šåŠ¡ç‰¹å®šçš„æ•°æ®æº

---

*é€šè¿‡SAGEæ„å»ºçš„RAGç³»ç»Ÿå…·å¤‡ç”Ÿäº§çº§çš„ç¨³å®šæ€§å’Œæ‰©å±•æ€§ï¼Œæ˜¯ä¼ä¸šçº§æ™ºèƒ½é—®ç­”åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚*
