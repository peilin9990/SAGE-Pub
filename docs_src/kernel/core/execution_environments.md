# æ‰§è¡Œç¯å¢ƒè¯¦è§£

SAGE Coreæ”¯æŒå¤šç§æ‰§è¡Œç¯å¢ƒï¼Œä»æœ¬åœ°å¼€å‘åˆ°åˆ†å¸ƒå¼éƒ¨ç½²ï¼Œæ¯ç§ç¯å¢ƒéƒ½æœ‰å…¶ç‰¹å®šçš„ç‰¹æ€§å’Œç”¨é€”ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å„ç§æ‰§è¡Œç¯å¢ƒçš„é…ç½®ã€ä½¿ç”¨å’Œæœ€ä½³å®è·µã€‚

## ğŸŒŸ ç¯å¢ƒæ¦‚è¿°

SAGE Coreæä¾›äº†ç»Ÿä¸€çš„æ‰§è¡Œç¯å¢ƒæŠ½è±¡ï¼Œç›®å‰æ”¯æŒï¼š

- **LocalEnvironment**: æœ¬åœ°å•æœºæ‰§è¡Œç¯å¢ƒ
- **RemoteEnvironment**: è¿œç¨‹åˆ†å¸ƒå¼æ‰§è¡Œç¯å¢ƒ

```mermaid
graph TB
    subgraph "SAGE æ‰§è¡Œç¯å¢ƒæ¶æ„"
        A[Application Code] --> B[Environment API]
        B --> C{Environment Type}
        
        C -->|Local| D[LocalEnvironment]
        C -->|Remote| E[RemoteEnvironment]
        
        D --> F[Local JobManager]
        E --> G[Remote JobManager Client]
        
        F --> H[Task Execution]
        G --> I[Distributed Task Execution]
    end
```

## ğŸ”§ æœ¬åœ°æ‰§è¡Œç¯å¢ƒ (LocalEnvironment)

### åŸºæœ¬ä½¿ç”¨

```python
from sage.core.api.local_environment import LocalEnvironment

# åˆ›å»ºæœ¬åœ°ç¯å¢ƒ
env = LocalEnvironment("my_local_app")

# é…ç½®é€‰é¡¹ï¼ˆé€šè¿‡configå­—å…¸ä¼ é€’ï¼‰
env = LocalEnvironment("my_local_app", config={
    "engine_host": "127.0.0.1",
    "engine_port": 19000
})

# è®¾ç½®æ§åˆ¶å°æ—¥å¿—ç­‰çº§
env.set_console_log_level("DEBUG")  # å¯é€‰: DEBUG, INFO, WARNING, ERROR

# åˆ›å»ºæ•°æ®æµï¼ˆä½¿ç”¨å®é™…å­˜åœ¨çš„æ–¹æ³•ï¼‰
# 1. ä»æ‰¹å¤„ç†æ•°æ®åˆ›å»º
data_stream = env.from_batch([1, 2, 3, 4, 5])

# 2. ä»Kafkaæºåˆ›å»º
kafka_stream = env.from_kafka_source(
    bootstrap_servers="localhost:9092",
    topic="my_topic",
    group_id="my_group"
)

# 3. ä»è‡ªå®šä¹‰æºå‡½æ•°åˆ›å»º
from sage.core.api.function.base_function import BaseFunction

class MySourceFunction(BaseFunction):
    def get_data_iterator(self):
        return iter(range(10))

source_stream = env.from_source(MySourceFunction)

# æ•°æ®å¤„ç† (ä½¿ç”¨å®é™…çš„DataStream API)
result = (data_stream
    .map(lambda x: x * 2)
    .filter(lambda x: x > 5)
    # .collect() æ–¹æ³•éœ€è¦æ ¹æ®å®é™…DataStream APIç¡®è®¤
)

# æäº¤ä»»åŠ¡åˆ°JobManager
env.submit()
```

### æœåŠ¡æ³¨å†Œ

```python
# æ³¨å†Œè‡ªå®šä¹‰æœåŠ¡
class MyCacheService:
    def __init__(self, cache_size=1000):
        self.cache_size = cache_size
        self.cache = {}

env.register_service("my_cache", MyCacheService, cache_size=1000)

# æ³¨å†Œæ•°æ®åº“è¿æ¥æœåŠ¡
class DatabaseConnection:
    def __init__(self, host, port, db):
        self.host = host
        self.port = port  
        self.db = db

env.register_service("db_conn", DatabaseConnection, 
                   host="localhost", port=5432, db="mydb")
```

### æ•°æ®æºåˆ›å»º

```python
# ä»å„ç§æ•°æ®ç±»å‹åˆ›å»ºæ‰¹å¤„ç†æµ
# 1. åˆ—è¡¨å’Œå…ƒç»„
data_list = ["item1", "item2", "item3", "item4", "item5"]
list_stream = env.from_batch(data_list)

# 2. ä»»ä½•å¯è¿­ä»£å¯¹è±¡
set_stream = env.from_batch({1, 2, 3, 4, 5})
range_stream = env.from_batch(range(100))
string_stream = env.from_batch("hello")  # æŒ‰å­—ç¬¦è¿­ä»£

# 3. è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
class CustomBatchFunction(BaseFunction):
    def get_data_iterator(self):
        return iter(range(50))
        
    def get_total_count(self):
        return 50

batch_stream = env.from_batch(CustomBatchFunction, custom_param="value")

# 4. Futureæµï¼ˆç”¨äºåé¦ˆå¾ªç¯ï¼‰
future_stream = env.from_future("feedback_loop")
```

## ğŸŒ è¿œç¨‹æ‰§è¡Œç¯å¢ƒ (RemoteEnvironment)

### åŸºæœ¬ä½¿ç”¨

```python
from sage.core.api.remote_environment import RemoteEnvironment

# åˆ›å»ºè¿œç¨‹ç¯å¢ƒï¼Œè¿æ¥åˆ°è¿œç¨‹JobManager
env = RemoteEnvironment(
    name="remote_app",
    config={
        "parallelism": 8,
        "buffer_size": 10000
    },
    host="127.0.0.1",  # JobManageræœåŠ¡ä¸»æœº
    port=19001         # JobManageræœåŠ¡ç«¯å£
)

# è®¾ç½®æ§åˆ¶å°æ—¥å¿—ç­‰çº§
env.set_console_log_level("INFO")

# åˆ›å»ºæ•°æ®æµï¼ˆä¸LocalEnvironmentç›¸åŒçš„APIï¼‰
data_stream = env.from_batch(list(range(1000)))

# æ•°æ®å¤„ç†
processed_stream = data_stream.map(lambda x: x * 2)

# æäº¤åˆ°è¿œç¨‹é›†ç¾¤
env_uuid = env.submit()
print(f"Job submitted with UUID: {env_uuid}")

# åœæ­¢è¿œç¨‹ä»»åŠ¡
response = env.stop()
print(f"Stop response: {response}")

# å…³é—­å¹¶æ¸…ç†è¿œç¨‹ç¯å¢ƒ
response = env.close()
print(f"Close response: {response}")
```

### è¿œç¨‹é…ç½®

```python
# è¯¦ç»†è¿œç¨‹é…ç½®
remote_env = RemoteEnvironment(
    name="production_app",
    config={
        # åŸºç¡€é…ç½®
        "parallelism": 16,
        "buffer_size": 50000,
        "checkpoint_interval": 300,  # 5åˆ†é’Ÿ
        
        # å®¹é”™é…ç½®
        "restart_strategy": "fixed-delay",
        "max_failures": 3,
        "failure_rate_interval": 60,
        
        # èµ„æºé…ç½®
        "taskmanager_memory": "2GB",
        "taskmanager_slots": 4
    },
    host="cluster-master.example.com",
    port=8081
)

# è·å–å®¢æˆ·ç«¯çŠ¶æ€
client = remote_env.client
print(f"Connected to JobManager at {client.host}:{client.port}")

# æäº¤å¹¶ç›‘æ§
env_uuid = remote_env.submit()
print(f"Remote job submitted: {env_uuid}")

# æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼ˆå¦‚æœJobManagerClientæ”¯æŒï¼‰
# status = client.get_job_status(env_uuid)
```

### é«˜çº§è¿œç¨‹æ“ä½œ

```python
# Kafkaæºåœ¨è¿œç¨‹ç¯å¢ƒä¸­çš„ä½¿ç”¨
kafka_stream = remote_env.from_kafka_source(
    bootstrap_servers="kafka1:9092,kafka2:9092",
    topic="events",
    group_id="remote_consumer",
    auto_offset_reset="earliest",
    buffer_size=20000
)

# å¤æ‚æ•°æ®å¤„ç†ç®¡é“
result_stream = (kafka_stream
    .map(parse_event_function)
    .filter(is_valid_event_function)
    .key_by(extract_key_function)
    .window(tumbling_time_window(minutes=5))
    .aggregate(count_aggregator)
)

# è¾“å‡ºåˆ°ç›®æ ‡ç³»ç»Ÿ
result_stream.add_sink(kafka_output_sink)

# æäº¤è¿œç¨‹ä»»åŠ¡
env_uuid = remote_env.submit()

# ä»»åŠ¡ç®¡ç†
try:
    # ç­‰å¾…ä»»åŠ¡è¿è¡Œä¸€æ®µæ—¶é—´
    import time
    time.sleep(300)  # è¿è¡Œ5åˆ†é’Ÿ
    
    # ä¼˜é›…åœæ­¢
    stop_response = remote_env.stop()
    if stop_response.get("status") == "success":
        print("Remote job stopped successfully")
    else:
        print(f"Stop failed: {stop_response}")
        
finally:
    # ç¡®ä¿èµ„æºæ¸…ç†
    remote_env.close()
```

## ï¿½ ç¯å¢ƒå¯¹æ¯”å’Œé€‰æ‹©æŒ‡å—

### ç¯å¢ƒç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | LocalEnvironment | RemoteEnvironment |
|------|------------------|-------------------|
| é€‚ç”¨åœºæ™¯ | å¼€å‘ã€æµ‹è¯•ã€å°è§„æ¨¡å¤„ç† | ç”Ÿäº§ã€å¤§è§„æ¨¡åˆ†å¸ƒå¼å¤„ç† |
| èµ„æºç®¡ç† | æœ¬åœ°èµ„æº | åˆ†å¸ƒå¼é›†ç¾¤èµ„æº |
| å®¹é”™èƒ½åŠ› | åŸºç¡€ | é«˜çº§ï¼ˆæ•…éšœæ¢å¤ã€é‡è¯•ï¼‰ |
| æ‰©å±•æ€§ | å•æœºé™åˆ¶ | æ°´å¹³æ‰©å±• |
| é…ç½®å¤æ‚åº¦ | ç®€å• | ç›¸å¯¹å¤æ‚ |
| è°ƒè¯•ä¾¿åˆ©æ€§ | é«˜ | ä¸­ç­‰ |

### é€‰æ‹©å»ºè®®

**é€‰æ‹© LocalEnvironment å½“ï¼š**
- å¼€å‘å’Œè°ƒè¯•åº”ç”¨ç¨‹åº
- æ•°æ®é‡è¾ƒå°ï¼ˆ< 1GBï¼‰
- å¤„ç†é€»è¾‘ç›¸å¯¹ç®€å•
- éœ€è¦å¿«é€ŸåŸå‹éªŒè¯

**é€‰æ‹© RemoteEnvironment å½“ï¼š**
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- å¤§æ•°æ®å¤„ç†ï¼ˆ> 1GBï¼‰
- éœ€è¦é«˜å¯ç”¨æ€§å’Œå®¹é”™
- éœ€è¦æ°´å¹³æ‰©å±•èƒ½åŠ›

### å®é™…ä½¿ç”¨ç¤ºä¾‹

```python
from sage.core.api.local_environment import LocalEnvironment
from sage.core.api.remote_environment import RemoteEnvironment

# å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨LocalEnvironment
def create_dev_environment():
    env = LocalEnvironment("dev_app")
    env.set_console_log_level("DEBUG")  # å¼€å‘æ—¶æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    return env

# ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨RemoteEnvironment  
def create_prod_environment():
    env = RemoteEnvironment(
        name="prod_app",
        config={
            "parallelism": 32,
            "checkpoint_interval": 300,
            "restart_strategy": "exponential-backoff"
        },
        host="prod-jobmanager.company.com",
        port=8081
    )
    env.set_console_log_level("INFO")  # ç”Ÿäº§ç¯å¢ƒå‡å°‘æ—¥å¿—è¾“å‡º
    return env

# æ ¹æ®è¿è¡Œæ¨¡å¼é€‰æ‹©ç¯å¢ƒ
import os
if os.getenv("SAGE_ENV") == "production":
    env = create_prod_environment()
else:
    env = create_dev_environment()

# ç›¸åŒçš„åº”ç”¨é€»è¾‘ä»£ç 
data_stream = env.from_batch(load_data())
result = data_stream.map(process_function).filter(filter_function)
env.submit()
```

## ï¿½ï¸ ç¯å¢ƒé…ç½®æœ€ä½³å®è·µ

### æ—¥å¿—é…ç½®

```python
# å¼€å‘ç¯å¢ƒï¼šè¯¦ç»†æ—¥å¿—
env = LocalEnvironment("dev_app")
env.set_console_log_level("DEBUG")

# æµ‹è¯•ç¯å¢ƒï¼šå…³é”®ä¿¡æ¯
env = LocalEnvironment("test_app")  
env.set_console_log_level("INFO")

# ç”Ÿäº§ç¯å¢ƒï¼šé”™è¯¯å’Œè­¦å‘Š
env = RemoteEnvironment("prod_app")
env.set_console_log_level("WARNING")
```

### æœåŠ¡æ³¨å†Œæ¨¡å¼

```python
# å•ä¾‹æœåŠ¡æ¨¡å¼
class DatabaseService:
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super().__new__(cls)
        return cls._instance
        
    def __init__(self, connection_string):
        if not hasattr(self, 'initialized'):
            self.connection_string = connection_string
            self.initialized = True

# æ³¨å†Œå•ä¾‹æœåŠ¡
env.register_service("db", DatabaseService, "postgresql://localhost:5432/mydb")

# å·¥å‚æ¨¡å¼æœåŠ¡
class CacheServiceFactory:
    @staticmethod
    def create_cache(cache_type="memory", **kwargs):
        if cache_type == "memory":
            return MemoryCache(**kwargs)
        elif cache_type == "redis":
            return RedisCache(**kwargs)
        else:
            raise ValueError(f"Unknown cache type: {cache_type}")

env.register_service("cache", CacheServiceFactory.create_cache, 
                   cache_type="redis", host="localhost", port=6379)
```

### é”™è¯¯å¤„ç†

```python
# ç¯å¢ƒçº§åˆ«çš„é”™è¯¯å¤„ç†
try:
    # åˆ›å»ºå’Œé…ç½®ç¯å¢ƒ
    env = RemoteEnvironment("my_app", host="jobmanager.example.com")
    
    # æ„å»ºæ•°æ®æµ
    stream = env.from_kafka_source("localhost:9092", "events", "group1")
    result = stream.map(processing_function)
    
    # æäº¤ä»»åŠ¡
    job_id = env.submit()
    print(f"Job submitted: {job_id}")
    
    # ç›‘æ§ä»»åŠ¡ï¼ˆå¯é€‰ï¼‰
    # monitor_job(job_id)
    
except Exception as e:
    print(f"Environment setup failed: {e}")
    # æ¸…ç†èµ„æº
    if 'env' in locals():
        env.close()
    raise
```

---

é€šè¿‡é€‰æ‹©åˆé€‚çš„æ‰§è¡Œç¯å¢ƒï¼Œæ‚¨å¯ä»¥ï¼š

- ğŸš€ **ä¼˜åŒ–æ€§èƒ½**: æ ¹æ®åº”ç”¨éœ€æ±‚é€‰æ‹©æœ¬åœ°æˆ–è¿œç¨‹æ‰§è¡Œ
- ğŸ”§ **ç®€åŒ–éƒ¨ç½²**: ä½¿ç”¨ç»Ÿä¸€APIåœ¨ä¸åŒç¯å¢ƒé—´æ— ç¼åˆ‡æ¢
- ğŸ“ˆ **å¼¹æ€§æ‰©å±•**: ä»æœ¬åœ°å¼€å‘æ‰©å±•åˆ°åˆ†å¸ƒå¼ç”Ÿäº§ç¯å¢ƒ
- ğŸ› ï¸ **çµæ´»é…ç½®**: é€šè¿‡æœåŠ¡æ³¨å†Œå’Œé…ç½®ç®¡ç†é€‚åº”ä¸åŒåœºæ™¯

é€‰æ‹©é€‚åˆæ‚¨åœºæ™¯çš„æ‰§è¡Œç¯å¢ƒï¼Œé‡Šæ”¾SAGE Coreçš„å…¨éƒ¨æ½œèƒ½ï¼

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [ç¯å¢ƒç®¡ç† API](../api/environments.md) - è¯¦ç»†çš„ç¯å¢ƒAPIæ–‡æ¡£
- <!-- [æ•°æ®æµå¤„ç†](./datastream_overview.md) -->
DataStream æ¦‚è§ˆ - æ•°æ®æµçš„åˆ›å»ºå’Œæ“ä½œ
- <!-- [æœåŠ¡ç®¡ç†](../services/service_management.md) -->
æœåŠ¡ç®¡ç† - æœåŠ¡æ³¨å†Œå’Œç®¡ç†
- <!-- [JobManager æ¶æ„](../jobmanager/architecture.md) -->
JobManager æ¶æ„ - JobManagerçš„å·¥ä½œåŸç†
