# æ•°æ®æµç¼–ç¨‹æ¨¡å‹

SAGE Core é‡‡ç”¨æ•°æ®æµç¼–ç¨‹æ¨¡å‹ï¼Œå°†è®¡ç®—é€»è¾‘è¡¨ç¤ºä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨è®¡ç®—æ“ä½œï¼Œè¾¹ä»£è¡¨æ•°æ®æµåŠ¨ã€‚è¿™ç§æ¨¡å‹ç‰¹åˆ«é€‚åˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„æ•°æ®ä¾èµ–å’Œå¼‚æ­¥æ‰§è¡Œéœ€æ±‚ã€‚

## ğŸ¯ æ ¸å¿ƒç†å¿µ

### å£°æ˜å¼ç¼–ç¨‹
ç”¨æˆ·ä¸“æ³¨äºæè¿°**è¦åšä»€ä¹ˆ**ï¼Œè€Œä¸æ˜¯**æ€ä¹ˆåš**ï¼š

```python
# å£°æ˜å¼ï¼šæè¿°æ•°æ®å¤„ç†é€»è¾‘
result = (env
    .from_source(input_source)
    .map(preprocess)      # é¢„å¤„ç†
    .map(embedding)       # å‘é‡åŒ–
    .map(retrieval)       # æ£€ç´¢
    .map(generation)      # ç”Ÿæˆ
    .sink(output_sink)    # è¾“å‡º
)

# å‘½ä»¤å¼ï¼šæè¿°æ‰§è¡Œæ­¥éª¤
data = read_from_source(input_source)
for item in data:
    processed = preprocess(item)
    embedded = embedding(processed)
    retrieved = retrieval(embedded)
    generated = generation(retrieved)
    write_to_sink(generated, output_sink)
```

### æ•°æ®é©±åŠ¨æ‰§è¡Œ
è®¡ç®—çš„è§¦å‘å®Œå…¨ç”±æ•°æ®å¯ç”¨æ€§é©±åŠ¨ï¼Œè€Œéç¨‹åºæ§åˆ¶æµï¼š

```python
# æ•°æ®åˆ°è¾¾è§¦å‘è®¡ç®—
source â†’ map1 â†’ filter â†’ map2 â†’ sink
  â†‘       â†‘       â†‘       â†‘      â†‘
 æ•°æ®    å¤„ç†    è¿‡æ»¤    å¤„ç†   è¾“å‡º
```

## ğŸ“Š æ•°æ®æµå›¾ç»“æ„

### åŸºæœ¬ç»„ä»¶

#### ğŸ”„ ç®—å­ (Operators)
ç®—å­æ˜¯æ•°æ®æµå›¾çš„èŠ‚ç‚¹ï¼Œæ‰§è¡Œå…·ä½“çš„è®¡ç®—é€»è¾‘ï¼š

```python
class MapOperator(BaseOperator):
    def __init__(self, func):
        self.func = func
    
    def process(self, input_data):
        return self.func(input_data)

class FilterOperator(BaseOperator):
    def __init__(self, predicate):
        self.predicate = predicate
    
    def process(self, input_data):
        return input_data if self.predicate(input_data) else None
```

#### ğŸ”— æ•°æ®æµ (DataStreams)
æ•°æ®æµæ˜¯ç®—å­ä¹‹é—´çš„è¿æ¥ï¼Œå®šä¹‰æ•°æ®ä¼ è¾“è·¯å¾„ï¼š

```python
class DataStream:
    def __init__(self, source_op, target_op):
        self.source = source_op
        self.target = target_op
        self.buffer = Queue()  # ç¼“å†²åŒº
    
    def send(self, data):
        self.buffer.put(data)
    
    def receive(self):
        return self.buffer.get()
```

### å›¾ç±»å‹

#### çº¿æ€§æµæ°´çº¿
æœ€ç®€å•çš„æ•°æ®æµæ¨¡å¼ï¼Œæ•°æ®æŒ‰é¡ºåºæµç»å„ä¸ªç®—å­ï¼š

```mermaid
graph LR
    A[Source] --> B[Map] --> C[Filter] --> D[Sink]
```

```python
# çº¿æ€§æµæ°´çº¿ç¤ºä¾‹
pipeline = (env
    .from_source(TextSource)
    .map(tokenizer)           # åˆ†è¯
    .map(embedding_model)     # åµŒå…¥
    .filter(quality_filter)   # è´¨é‡è¿‡æ»¤
    .sink(vector_store)       # å­˜å‚¨
)
```

#### åˆ†æ”¯åˆå¹¶æµ
æ”¯æŒæ•°æ®æµçš„åˆ†æ”¯å’Œåˆå¹¶ï¼š

```mermaid
graph LR
    A[Source] --> B[Split]
    B --> C[Branch 1]
    B --> D[Branch 2]
    C --> E[Merge]
    D --> E
    E --> F[Sink]
```

```python
# åˆ†æ”¯åˆå¹¶ç¤ºä¾‹
main_stream = env.from_source(QuerySource)

# åˆ†æ”¯1ï¼šæ£€ç´¢
retrieval_stream = main_stream.map(EmbeddingEncoder).map(VectorRetriever)

# åˆ†æ”¯2ï¼šç¼“å­˜æŸ¥è¯¢
cache_stream = main_stream.map(CacheQuery)

# ä¸¤ä¸ªåˆ†æ”¯å¯ä»¥åˆ†åˆ«å¤„ç†å¹¶è¾“å‡ºåˆ°ä¸åŒçš„sink
retrieval_stream.map(ResponseGenerator).sink(ResponseSink)
cache_stream.sink(CacheSink)
```

### è¿æ¥æµ
å¤šä¸ªæ•°æ®æµçš„ååŒå¤„ç†ï¼š

```mermaid
graph LR
    A[Stream 1] --> C[Connect]
    B[Stream 2] --> C
    C --> D[CoMap] --> E[Sink]
```

```python
# è¿æ¥æµç¤ºä¾‹
query_stream = env.from_source(QuerySource)
context_stream = env.from_source(ContextSource)

# è¿æ¥ä¸¤ä¸ªæµè¿›è¡ŒååŒå¤„ç†
connected = query_stream.connect(context_stream)
result = connected.comap(rag_processor).sink(ResponseSink)
```

## ğŸ”§ API è®¾è®¡åŸåˆ™

### é“¾å¼è°ƒç”¨
æ”¯æŒæ–¹æ³•é“¾å¼è°ƒç”¨ï¼Œæå‡ä»£ç å¯è¯»æ€§ï¼š

```python
# è‰¯å¥½çš„é“¾å¼è®¾è®¡
result = (stream
    .map(func1)
    .filter(pred1)
    .map(func2)
    .keyby(key_selector)
    .map(aggregator)
    .sink(output)
)
```

### ç±»å‹å®‰å…¨
åŸºäºPythonç±»å‹æç¤ºæä¾›ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥ï¼š

```python
from typing import TypeVar, Generic

T = TypeVar('T')
U = TypeVar('U')

class DataStream(Generic[T]):
    def map(self, func: Callable[[T], U]) -> 'DataStream[U]':
        return DataStream[U](MapOperator(func))
    
    def filter(self, predicate: Callable[[T], bool]) -> 'DataStream[T]':
        return DataStream[T](FilterOperator(predicate))
```

### å»¶è¿Ÿæ‰§è¡Œ
æ„å»ºé˜¶æ®µåªå®šä¹‰è®¡ç®—å›¾ï¼Œæ‰§è¡Œé˜¶æ®µæ‰å¼€å§‹è®¡ç®—ï¼š

```python
# æ„å»ºé˜¶æ®µï¼šå®šä¹‰è®¡ç®—å›¾
pipeline = (env
    .from_source(source)
    .map(processor)
    .sink(output)
)  # æ­¤æ—¶è¿˜æ²¡æœ‰æ‰§è¡Œä»»ä½•è®¡ç®—

# æ‰§è¡Œé˜¶æ®µï¼šå¯åŠ¨è®¡ç®—
env.submit()  # å¼€å§‹æ‰§è¡Œæµæ°´çº¿
```

## ğŸƒ æ‰§è¡Œæ¨¡å‹

### å¼‚æ­¥æ‰§è¡Œ
ç®—å­ä¹‹é—´å¼‚æ­¥æ‰§è¡Œï¼Œæé«˜èµ„æºåˆ©ç”¨ç‡ï¼š

```python
class AsyncOperator:
    async def process(self, data):
        result = await self.async_operation(data)
        return result
    
    async def run(self):
        while True:
            data = await self.input_queue.get()
            result = await self.process(data)
            await self.output_queue.put(result)
```

### èƒŒå‹å¤„ç†
è‡ªåŠ¨å¤„ç†ä¸Šä¸‹æ¸¸å¤„ç†é€Ÿåº¦å·®å¼‚ï¼š

```python
class BackpressureQueue:
    def __init__(self, max_size=1000):
        self.queue = asyncio.Queue(maxsize=max_size)
    
    async def put(self, item):
        if self.queue.full():
            # èƒŒå‹ä¿¡å·ï¼šé€šçŸ¥ä¸Šæ¸¸å‡ç¼“å‘é€
            await self.notify_backpressure()
        await self.queue.put(item)
```

### çŠ¶æ€ç®¡ç†
ç®—å­å¯ä»¥åœ¨å¤„ç†è¿‡ç¨‹ä¸­ç»´æŠ¤å†…éƒ¨çŠ¶æ€ï¼š

```python
import time
from sage.core.api.function.base_function import BaseFunction

class CounterFunction(BaseFunction):
    def __init__(self):
        super().__init__()
        self.counter = 0  # å†…éƒ¨çŠ¶æ€
    
    def process(self, data):
        self.counter += 1
        return {
            'data': data,
            'count': self.counter,
            'timestamp': time.time()
        }
```

## ğŸ¨ å¸¸ç”¨æ¨¡å¼

### Map æ¨¡å¼
ä¸€å¯¹ä¸€çš„æ•°æ®è½¬æ¢ï¼š

```python
# æ–‡æœ¬é¢„å¤„ç†
text_stream = (env
    .from_source(TextSource)
    .map(lambda x: x.lower())          # è½¬å°å†™
    .map(lambda x: x.strip())          # å»ç©ºæ ¼
    .map(tokenize)                     # åˆ†è¯
)
```

### Filter æ¨¡å¼
æ•°æ®è¿‡æ»¤ï¼š

```python
# è´¨é‡è¿‡æ»¤
quality_stream = (env
    .from_source(DataSource)
    .filter(lambda x: len(x.text) > 10)     # é•¿åº¦è¿‡æ»¤
    .filter(lambda x: x.score > 0.8)        # åˆ†æ•°è¿‡æ»¤
    .filter(is_valid_format)                 # æ ¼å¼è¿‡æ»¤
)
```

### FlatMap æ¨¡å¼
ä¸€å¯¹å¤šçš„æ•°æ®è½¬æ¢ï¼š

```python
# æ–‡æ¡£åˆ†å—
chunk_stream = (env
    .from_source(DocumentSource)
    .flatmap(lambda doc: split_into_chunks(doc))  # åˆ†å—
    .map(create_embedding)                         # å‘é‡åŒ–
)
```

### KeyBy æ¨¡å¼
æŒ‰é”®åˆ†ç»„æ•°æ®ï¼š

```python
# ç”¨æˆ·äº‹ä»¶åˆ†ç»„
grouped_stream = (env
    .from_source(EventSource)
    .keyby(lambda event: event.user_id)     # æŒ‰ç”¨æˆ·åˆ†ç»„
    .map(user_event_processor)              # å¤„ç†ç”¨æˆ·äº‹ä»¶
)
```

### Connect æ¨¡å¼
å¤šæµè¿æ¥ï¼š

```python
# ç”¨æˆ·æŸ¥è¯¢ä¸ä¸Šä¸‹æ–‡è¿æ¥
connected_stream = query_stream.connect(context_stream)
result = connected_stream.comap(merge_query_context)    # åˆå¹¶é€»è¾‘
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### æ‰§è¡Œå›¾ä¼˜åŒ–
SAGE Coreåœ¨æ‰§è¡Œå‰ä¼šå¯¹æ•°æ®æµå›¾è¿›è¡Œä¼˜åŒ–ï¼š

```python
# é“¾å¼mapæ“ä½œä¼šè¢«è‡ªåŠ¨ä¼˜åŒ–
stream.map(func1).map(func2).map(func3)

# ç¼–è¯‘å™¨ä¼šåˆ†æå¹¶ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’
env.submit()  # ä¼˜åŒ–åœ¨æäº¤æ—¶è‡ªåŠ¨è¿›è¡Œ
```

### å¹¶è¡Œæ‰§è¡Œ
æ”¯æŒç®—å­å¹¶è¡Œæ‰§è¡Œï¼š

```python
# é€šè¿‡ç¯å¢ƒé…ç½®å¹¶è¡Œåº¦
env.config['parallelism'] = 4  # è®¾ç½®å…¨å±€å¹¶è¡Œåº¦
stream.map(expensive_operation)
```

### æ•°æ®å¤„ç†
SAGE Coreæä¾›äº†åŸºç¡€çš„æ‰¹é‡å¤„ç†èƒ½åŠ›ï¼š

```python
# æ‰¹é‡æ•°æ®å¤„ç†
stream.map(operation_function)
```

## ğŸ› ï¸ è°ƒè¯•ä¸ç›‘æ§

### æ—¥å¿—è®°å½•
SAGE Coreæä¾›äº†å†…ç½®çš„æ—¥å¿—è®°å½•åŠŸèƒ½ï¼š

```python
# é€šè¿‡printæ–¹æ³•æŸ¥çœ‹æ•°æ®æµ
stream.map(processor).print("è°ƒè¯•è¾“å‡º").sink(output)

# è®¾ç½®æ—¥å¿—ç­‰çº§
env.set_console_log_level("DEBUG")
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. å‡½æ•°è®¾è®¡
- ä¿æŒå‡½æ•°çº¯å‡€æ€§ï¼Œé¿å…å‰¯ä½œç”¨
- ä¼˜å…ˆä½¿ç”¨ä¸å¯å˜æ•°æ®ç»“æ„
- åˆç†è®¾ç½®è¶…æ—¶æ—¶é—´

```python
# è‰¯å¥½çš„å‡½æ•°è®¾è®¡
def process_text(text: str) -> ProcessedText:
    # çº¯å‡½æ•°ï¼Œæ— å‰¯ä½œç”¨
    return ProcessedText(
        content=text.strip().lower(),
        word_count=len(text.split()),
        timestamp=datetime.now()
    )
```

### 2. é”™è¯¯å¤„ç†
- ä½¿ç”¨try-catchåŒ…è£…å¯èƒ½å¤±è´¥çš„æ“ä½œ
- è®¾ç½®åˆç†çš„é‡è¯•ç­–ç•¥
- æä¾›å›é€€æœºåˆ¶

```python
def robust_processor(data):
    try:
        return expensive_operation(data)
    except ProcessingError as e:
        # è®°å½•é”™è¯¯
        logger.error(f"Processing failed: {e}")
        # è¿”å›é»˜è®¤å€¼æˆ–None
        return None
```

### 3. èµ„æºç®¡ç†
- åˆç†è®¾ç½®ç¯å¢ƒé…ç½®
- åŠæ—¶é‡Šæ”¾ä¸å†éœ€è¦çš„èµ„æº
- é€šè¿‡ç¯å¢ƒé…ç½®ç®¡ç†å¤–éƒ¨è¿æ¥

```python
# ç¯å¢ƒé…ç½®
env.config.update({
    'buffer_size': 1000,
    'timeout': 5000,
    'max_memory': "2GB"
})
```

---

**ä¸‹ä¸€æ­¥**: äº†è§£ <!-- [æµæ°´çº¿ç¼–æ’ç³»ç»Ÿ](./pipeline_orchestration.md) -->
æµæ°´çº¿ç¼–æ’ç³»ç»Ÿ å¦‚ä½•ç®¡ç†å¤æ‚çš„æ•°æ®æµæ‰§è¡Œã€‚
