# SAGE Core æ•°æ®æµç¼–ç¨‹æ¨¡å‹æ·±åº¦è§£æ

SAGE Core é‡‡ç”¨å…ˆè¿›çš„æ•°æ®æµç¼–ç¨‹æ¨¡å‹ï¼Œå°†è®¡ç®—é€»è¾‘è¡¨ç¤ºä¸ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨è®¡ç®—æ“ä½œï¼Œè¾¹ä»£è¡¨æ•°æ®æµåŠ¨ã€‚è¿™ç§æ¨¡å‹ç‰¹åˆ«é€‚åˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†åœºæ™¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚çš„æ•°æ®ä¾èµ–å’Œå¼‚æ­¥æ‰§è¡Œéœ€æ±‚ã€‚

## ğŸ¯ æ ¸å¿ƒç†å¿µä¸ä¼˜åŠ¿

### å£°æ˜å¼ vs å‘½ä»¤å¼ç¼–ç¨‹

| ç‰¹æ€§ | å£°æ˜å¼ç¼–ç¨‹ (SAGE) | å‘½ä»¤å¼ç¼–ç¨‹ (ä¼ ç»Ÿ) |
|------|-------------------|------------------|
| **å…³æ³¨ç‚¹** | æè¿°"åšä»€ä¹ˆ" | æè¿°"å¦‚ä½•åš" |
| **ä»£ç é£æ ¼** | é“¾å¼è°ƒç”¨ï¼Œç®€æ´ä¼˜é›… | é¡ºåºè¯­å¥ï¼Œè¯¦ç»†æ­¥éª¤ |
| **ä¼˜åŒ–ç©ºé—´** | è‡ªåŠ¨ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’ | æ‰‹åŠ¨ä¼˜åŒ–ä»£ç é€»è¾‘ |
| **å¯ç»´æŠ¤æ€§** | é«˜å±‚æŠ½è±¡ï¼Œæ˜“äºç†è§£ | åº•å±‚ç»†èŠ‚ï¼Œç»´æŠ¤å¤æ‚ |
| **å¹¶è¡ŒåŒ–** | è‡ªåŠ¨å¹¶è¡Œä¼˜åŒ– | æ‰‹åŠ¨çº¿ç¨‹ç®¡ç† |

```python
# å£°æ˜å¼ç¼–ç¨‹ç¤ºä¾‹ - ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘
result = (env
    .from_source(input_source)
    .map(preprocess)          # æ•°æ®é¢„å¤„ç†
    .map(embedding)           # å‘é‡åµŒå…¥è®¡ç®—
    .map(retrieval)           # å‘é‡æ£€ç´¢
    .map(generation)          # æ–‡æœ¬ç”Ÿæˆ
    .sink(output_sink)        # ç»“æœè¾“å‡º
)

# å‘½ä»¤å¼ç¼–ç¨‹ç¤ºä¾‹ - éœ€è¦ç®¡ç†æ‰§è¡Œç»†èŠ‚
def process_data():
    data = read_from_source(input_source)
    results = []
    for item in data:
        processed = preprocess(item)        # æ‰‹åŠ¨å¾ªç¯å¤„ç†
        embedded = embedding(processed)    # æ˜¾å¼çŠ¶æ€ç®¡ç†
        retrieved = retrieval(embedded)     # æ‰‹åŠ¨é”™è¯¯å¤„ç†
        generated = generation(retrieved)   # å¤æ‚çš„æµç¨‹æ§åˆ¶
        results.append(generated)
    write_to_sink(results, output_sink)    # ç»“æœè¾“å‡ºç®¡ç†
```

### æ•°æ®é©±åŠ¨æ‰§è¡Œæ¨¡å‹

SAGE Core çš„æ‰§è¡Œå®Œå…¨ç”±æ•°æ®å¯ç”¨æ€§é©±åŠ¨ï¼Œå®ç°äº†é«˜æ•ˆçš„å¼‚æ­¥å¤„ç†ï¼š

```mermaid
flowchart TD
    A[æ•°æ®æº Source] --> B[é¢„å¤„ç† Map]
    B --> C[è´¨é‡è¿‡æ»¤ Filter]
    C --> D[å‘é‡è®¡ç®— Map]
    D --> E[ç»“æœè¾“å‡º Sink]
    
    subgraph æ•°æ®é©±åŠ¨æ‰§è¡Œ
        F[æ•°æ®åˆ°è¾¾] --> G[è§¦å‘å¤„ç†]
        G --> H[å¼‚æ­¥ä¼ é€’]
        H --> I[ç»§ç»­ä¸‹ä¸€é˜¶æ®µ]
    end
    
    style F fill:#e1f5fe
    style G fill:#bbdefb
    style H fill:#90caf9
    style I fill:#64b5f6
```

## ğŸ“Š æ•°æ®æµå›¾ç»“æ„ä¸ç»„ä»¶ä½“ç³»

### æ ¸å¿ƒæ¶æ„ç»„ä»¶

```mermaid
classDiagram
    class BaseOperator {
        +String name
        +Integer parallelism
        +process(input_data)
        +open()
        +close()
    }
    
    class DataStream {
        -List~BaseOperator~ operators
        -Map~String, Object~ config
        +map(func)
        +filter(predicate)
        +keyby(key_selector)
        +sink(sink_func)
    }
    
    class SourceOperator {
        +read()
        +assign_timestamps()
        +create_watermarks()
    }
    
    class TransformOperator {
        +process_element()
        +on_timer()
    }
    
    class SinkOperator {
        +write()
        +flush()
        +commit()
    }
    
    BaseOperator <|-- SourceOperator
    BaseOperator <|-- TransformOperator
    BaseOperator <|-- SinkOperator
    DataStream *-- BaseOperator
```

### ç®—å­ç±»å‹è¯¦è§£

#### 1. æ•°æ®æºç®—å­ (Source Operators)
```python
class KafkaSourceOperator(SourceOperator):
    def __init__(self, bootstrap_servers, topics, group_id):
        self.consumer = KafkaConsumer(
            bootstrap_servers=bootstrap_servers,
            topics=topics,
            group_id=group_id
        )
    
    def read(self):
        """ä»KafkaæŒç»­è¯»å–æ•°æ®"""
        while self.running:
            records = self.consumer.poll(timeout_ms=100)
            for record in records:
                yield record.value
    
    def assign_timestamps(self, record):
        """åˆ†é…æ—¶é—´æˆ³ç”¨äºäº‹ä»¶æ—¶é—´å¤„ç†"""
        return record.timestamp if hasattr(record, 'timestamp') else time.time()
```

#### 2. è½¬æ¢ç®—å­ (Transformation Operators)
```python
class SmartMapOperator(TransformOperator):
    def __init__(self, user_func, config=None):
        self.user_func = user_func
        self.config = config or {}
        self.metrics = {}  # æ€§èƒ½æŒ‡æ ‡æ”¶é›†
        
    def process_element(self, value, ctx):
        start_time = time.time()
        try:
            result = self.user_func(value)
            # è®°å½•å¤„ç†å»¶è¿Ÿ
            self.metrics['latency'] = time.time() - start_time
            self.metrics['processed_count'] += 1
            return result
        except Exception as e:
            self.metrics['error_count'] += 1
            ctx.output_error(value, e)  # é”™è¯¯å¤„ç†
            return None
```

#### 3. æ•°æ®æ±‡ç®—å­ (Sink Operators)
```python
class ElasticsearchSinkOperator(SinkOperator):
    def __init__(self, hosts, index_name, batch_size=1000):
        self.es_client = Elasticsearch(hosts)
        self.index_name = index_name
        self.batch_size = batch_size
        self.buffer = []
        
    def write(self, record):
        """æ‰¹é‡å†™å…¥ä¼˜åŒ–"""
        self.buffer.append(record)
        if len(self.buffer) >= self.batch_size:
            self.flush()
            
    def flush(self):
        """æ‰¹é‡æäº¤æ•°æ®"""
        if self.buffer:
            bulk_data = [
                {"index": {"_index": self.index_name}}
                for record in self.buffer
            ]
            self.es_client.bulk(bulk_data)
            self.buffer.clear()
```

## ğŸ”§ é«˜çº§æ•°æ®æµæ¨¡å¼

### 1. å¤æ‚äº‹ä»¶å¤„ç†æ¨¡å¼
```python
# å¤æ‚äº‹ä»¶æ£€æµ‹æµæ°´çº¿
cep_pipeline = (env
    .from_source(EventSource("user-events"))
    .assign_timestamps_and_watermarks(
        WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(5))
    )
    .key_by(lambda event: event.user_id)
    .window(SlidingEventTimeWindows.of(Duration.of_minutes(10), Duration.of_minutes(1)))
    .process(ComplexEventProcessor(), name="complex-event-detector")
    .sink(AlertSink("alerts-topic"))
)

class ComplexEventProcessor(ProcessWindowFunction):
    def process(self, key, context, events):
        # æ£€æµ‹å¤æ‚äº‹ä»¶æ¨¡å¼
        pattern = self.detect_pattern(events)
        if pattern:
            yield {
                "user_id": key,
                "pattern_type": pattern.type,
                "timestamp": context.window().end(),
                "events_count": len(events)
            }
```

### 2. çŠ¶æ€æµå¤„ç†æ¨¡å¼
```python
# æœ‰çŠ¶æ€æµå¤„ç†ç¤ºä¾‹
stateful_pipeline = (env
    .from_source(UserBehaviorSource())
    .key_by(lambda x: x["user_id"])
    .map(UserSessionAggregator(), name="session-aggregator")
    .map(BehaviorAnalyzer(), name="behavior-analyzer")
    .sink(ProfileUpdaterSink())
)

class UserSessionAggregator(MapFunction):
    def __init__(self):
        self.session_state = None
        
    def open(self, context):
        # åˆå§‹åŒ–çŠ¶æ€æè¿°ç¬¦
        state_descriptor = ValueStateDescriptor(
            "user_session", 
            Types.POJO(UserSession)
        )
        self.session_state = context.get_keyed_state(state_descriptor)
        
    def map(self, event):
        current_session = self.session_state.value() or UserSession(event.user_id)
        current_session.update(event)
        self.session_state.update(current_session)
        return current_session
```

### 3. æœºå™¨å­¦ä¹ æ¨ç†æµæ°´çº¿
```python
# å¤§è¯­è¨€æ¨¡å‹æ¨ç†æµæ°´çº¿
llm_pipeline = (env
    .from_source(QuerySource("user-queries"))
    .map(QueryPreprocessor(), name="query-preprocessor")
    .map(EmbeddingGenerator("model/embedding"), 
         name="embedding-generator")
    .map(ContextRetriever("vector-db"), 
         name="context-retriever")
    .map(LLMInferenceEngine("model/llm"), 
         name="llm-inference")
    .map(ResponsePostprocessor(), 
         name="response-postprocessor")
    .sink(ResponseSink("response-topic"))
)

class LLMInferenceEngine(MapFunction):
    def __init__(self, model_path):
        self.model_path = model_path
        self.model = None
        self.batch_size = 32
        
    def open(self, context):
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        self.model = load_llm_model(self.model_path)
        
    def map(self, input_batch):
        # æ‰¹é‡æ¨ç†ä¼˜åŒ–
        if isinstance(input_batch, list):
            return self.model.generate_batch(input_batch)
        else:
            return self.model.generate(input_batch)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### æ‰§è¡Œä¼˜åŒ–æŠ€æœ¯å¯¹æ¯”

| ä¼˜åŒ–æŠ€æœ¯ | é€‚ç”¨åœºæ™¯ | 
|---------|---------|---------|
| **ç®—å­èåˆ** | ç›¸é‚»æ— çŠ¶æ€ç®—å­ | 
| **æ•°æ®æœ¬åœ°åŒ–** | æ•°æ®å¯†é›†å‹åº”ç”¨ | 
| **æ‰¹é‡å¤„ç†** | é«˜åååœºæ™¯ | 
| **å¼‚æ­¥I/O** | I/Oå¯†é›†å‹åº”ç”¨ | 
| **çŠ¶æ€åˆ†åŒº** | æœ‰çŠ¶æ€è®¡ç®— | 

### ä¼˜åŒ–é…ç½®ç¤ºä¾‹
```python
# é«˜æ€§èƒ½æµæ°´çº¿é…ç½®
optimized_env = StreamExecutionEnvironment.create(
    execution_mode=ExecutionMode.PIPELINED,
    parallelism=16,  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    buffer_timeout=50,  # æ¯«ç§’
    object_reuse=True,
    state_backend=StateBackend.ROCKSDB,
    checkpoint_config=CheckpointConfig(
        interval=30000,  # 30ç§’
        timeout=60000,
        min_pause_between_checkpoints=5000
    )
)

# å†…å­˜ä¼˜åŒ–é…ç½®
memory_config = {
    'taskmanager.memory.process.size': '4gb',
    'taskmanager.memory.network.min': '64mb',
    'taskmanager.memory.network.max': '1gb',
    'taskmanager.memory.managed.fraction': '0.4'
}
```

## ğŸ› ï¸ è°ƒè¯•ä¸ç›‘æ§æœ€ä½³å®è·µ

### 1. åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ
```python
# OpenTelemetryé›†æˆç¤ºä¾‹
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider

def setup_tracing():
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)
    
    # æ·»åŠ å¯¼å‡ºå™¨
    otlp_exporter = OTLPSpanExporter()
    span_processor = BatchSpanProcessor(otlp_exporter)
    tracer_provider.add_span_processor(span_processor)
    
    return trace.get_tracer("sage-pipeline")

class TracedOperator(BaseOperator):
    def __init__(self, name):
        self.tracer = setup_tracing()
        self.span_name = name
        
    def process(self, data):
        with self.tracer.start_as_current_span(self.span_name) as span:
            span.set_attribute("data.size", len(str(data)))
            result = self._process_impl(data)
            span.set_attribute("result.size", len(str(result)))
            return result
```

### 2. æ€§èƒ½ç›‘æ§é…ç½®
```yaml
# ç›‘æ§é…ç½®
monitoring:
  metrics:
    reporters:
      - type: prometheus
        port: 9090
        interval: 10s
      - type: jmx
        port: 9091
    system_metrics: true
    user_metrics: true
    
  logging:
    level: INFO
    format: json
    exporters:
      - type: elasticsearch
        hosts: ["http://elk:9200"]
        index: "sage-logs"
        
  alerting:
    rules:
      - metric: throughput
        condition: < 1000 records/s for 5m
        severity: WARNING
      - metric: latency_p99
        condition: > 500ms for 2m
        severity: CRITICAL
```

## ğŸ“‹ ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 1. å®¹é”™è®¾è®¡
```python
# å®¹é”™é…ç½®
fault_tolerance_config = {
    'restart-strategy': 'exponential-delay',
    'restart-strategy.exponential-delay.initial-backoff': '10s',
    'restart-strategy.exponential-delay.max-backoff': '5m',
    'restart-strategy.exponential-delay.backoff-multiplier': '2.0',
    'checkpointing': 'exactly_once',
    'checkpointing.interval': '30s',
    'checkpointing.timeout': '5m',
    'checkpointing.min-pause': '10s'
}

# çŠ¶æ€åç«¯é…ç½®
state_backend_config = {
    'backend': 'rocksdb',
    'rocksdb.localdir': '/tmp/rocksdb',
    'rocksdb.compaction.style': 'universal',
    'rocksdb.writebuffer.size': '64MB',
    'rocksdb.max.write.buffer.number': '4'
}
```

### 2. å®‰å…¨é…ç½®
```python
# å®‰å…¨é…ç½®ç¤ºä¾‹
security_config = {
    'ssl.enabled': True,
    'ssl.keystore.path': '/path/to/keystore',
    'ssl.keystore.password': 'changeit',
    'ssl.truststore.path': '/path/to/truststore',
    'ssl.truststore.password': 'changeit',
    'authentication.type': 'kerberos',
    'authorization.enabled': True
}

# æ•°æ®åŠ å¯†
class EncryptedSinkOperator(SinkOperator):
    def __init__(self, inner_sink, encryption_key):
        self.inner_sink = inner_sink
        self.encryption_key = encryption_key
        
    def write(self, record):
        encrypted_data = self.encrypt(record, self.encryption_key)
        self.inner_sink.write(encrypted_data)
        
    def encrypt(self, data, key):
        # å®ç°åŠ å¯†é€»è¾‘
        cipher = AES.new(key, AES.MODE_GCM)
        ciphertext, tag = cipher.encrypt_and_digest(data.encode())
        return cipher.nonce + tag + ciphertext
```

---

**ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘**:
- <!-- ./state_management.md -->
æ·±å…¥ç†è§£çŠ¶æ€ç®¡ç† - æŒæ¡æœ‰çŠ¶æ€è®¡ç®—çš„æ ¸å¿ƒæ¦‚å¿µ
- <!-- ./performance_tuning.md -->
æ€§èƒ½è°ƒä¼˜å®æˆ˜ - å­¦ä¹ ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- <!-- ./deployment_guide.md -->
åˆ†å¸ƒå¼éƒ¨ç½²æŒ‡å— - äº†è§£é›†ç¾¤éƒ¨ç½²å’Œç®¡ç†

é€šè¿‡æ·±å…¥æŒæ¡SAGE Coreçš„æ•°æ®æµç¼–ç¨‹æ¨¡å‹ï¼Œæ‚¨å°†èƒ½å¤Ÿæ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æµæ°´çº¿ï¼Œæ»¡è¶³å„ç§å¤æ‚çš„ä¸šåŠ¡åœºæ™¯éœ€æ±‚ã€‚

---

**ä¸‹ä¸€æ­¥**: äº†è§£ <!-- [æµæ°´çº¿ç¼–æ’ç³»ç»Ÿ](./pipeline_orchestration.md) -->
æµæ°´çº¿ç¼–æ’ç³»ç»Ÿ å¦‚ä½•ç®¡ç†å¤æ‚çš„æ•°æ®æµæ‰§è¡Œã€‚
